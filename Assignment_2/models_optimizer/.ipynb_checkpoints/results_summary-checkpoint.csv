Optimizer,Learning Rate,Scheduler,Final Train Loss,Final Val Loss
SGD,0.001,None,0.29887706822445315,0.33161464237397714
SGD,0.001,StepLR,0.5375624725054349,0.5897481854884855
SGD,0.001,ReduceLROnPlateau,0.30675050348884636,0.3382012161516374
SGD,0.01,None,0.2709303019388068,0.31074275521020733
SGD,0.01,StepLR,0.3527813675182481,0.38598860824300396
SGD,0.01,ReduceLROnPlateau,0.25440382660036126,0.29497745080340293
SGD,0.1,None,0.2282420642253372,0.2967654137361434
SGD,0.1,StepLR,0.26635938107727036,0.3019932063837205
SGD,0.1,ReduceLROnPlateau,,
SGD_Momentum,0.001,None,0.2752980921838072,0.3078073281193933
SGD_Momentum,0.001,StepLR,0.3770203512282141,0.4067480448753603
SGD_Momentum,0.001,ReduceLROnPlateau,0.25067532651366725,0.2960531663510107
SGD_Momentum,0.01,None,0.27444566041231155,0.3215142375519199
SGD_Momentum,0.01,StepLR,0.2592863070988847,0.29283782539348446
SGD_Momentum,0.01,ReduceLROnPlateau,0.2706791874201548,0.30350088928976365
SGD_Momentum,0.1,None,,
SGD_Momentum,0.1,StepLR,0.7994204169559863,0.830846770636497
SGD_Momentum,0.1,ReduceLROnPlateau,,
Adam,0.001,None,0.22634862997238675,0.28663637236722056
Adam,0.001,StepLR,0.29742956308708074,0.3252462357763321
Adam,0.001,ReduceLROnPlateau,0.23109668150784507,0.2826287313815086
Adam,0.01,None,0.21680476849958782,0.28354520494899443
Adam,0.01,StepLR,0.2561143650823543,0.2923051916783856
Adam,0.01,ReduceLROnPlateau,0.20236174191438383,0.28217670441635195
Adam,0.1,None,0.37072656243558855,0.41565052275696107
Adam,0.1,StepLR,0.28631147889480474,0.3254181914512188
Adam,0.1,ReduceLROnPlateau,0.2875826400854895,0.32974162854013905
