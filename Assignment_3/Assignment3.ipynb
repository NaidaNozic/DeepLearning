{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Autoregressive Language Modeling with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: torch==2.4.1 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Collecting datasets==3.1.0\n",
      "  Obtaining dependency information for datasets==3.1.0 from https://files.pythonhosted.org/packages/ed/a5/33cf000137545a08b0a3a6ea76c8ccbd87917f78bb5d737f9f56f3b11ef6/datasets-3.1.0-py3-none-any.whl.metadata\n",
      "  Using cached datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from torch==2.4.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from torch==2.4.1) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from torch==2.4.1) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from torch==2.4.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from torch==2.4.1) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from torch==2.4.1) (2023.10.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from datasets==3.1.0) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from datasets==3.1.0) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from datasets==3.1.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from datasets==3.1.0) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from datasets==3.1.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from datasets==3.1.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from datasets==3.1.0) (3.5.0)\n",
      "Collecting multiprocess<0.70.17 (from datasets==3.1.0)\n",
      "  Obtaining dependency information for multiprocess<0.70.17 from https://files.pythonhosted.org/packages/50/15/b56e50e8debaf439f44befec5b2af11db85f6e0f344c3113ae0be0593a91/multiprocess-0.70.16-py311-none-any.whl.metadata\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from datasets==3.1.0) (3.9.3)\n",
      "Collecting huggingface-hub>=0.23.0 (from datasets==3.1.0)\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/61/8c/fbdc0a88a622d9fa54e132d7bf3ee03ec602758658a2db5b339a65be2cfe/huggingface_hub-0.27.0-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from datasets==3.1.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from datasets==3.1.0) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from aiohttp->datasets==3.1.0) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from aiohttp->datasets==3.1.0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from aiohttp->datasets==3.1.0) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from aiohttp->datasets==3.1.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from aiohttp->datasets==3.1.0) (1.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets==3.1.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets==3.1.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets==3.1.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets==3.1.0) (2024.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets==3.1.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from jinja2->torch==2.4.1) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from pandas->datasets==3.1.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from pandas->datasets==3.1.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from pandas->datasets==3.1.0) (2023.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from sympy->torch==2.4.1) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\crewb\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==3.1.0) (1.16.0)\n",
      "Using cached datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Using cached huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Installing collected packages: multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "Successfully installed datasets-3.1.0 huggingface-hub-0.27.0 multiprocess-0.70.16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==2.4.1 datasets==3.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "class MovieDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, block_size=256):\n",
    "        # Load dataset\n",
    "        ds = load_dataset(\"Pablinho/movies-dataset\")\n",
    "        data = ds['train'].to_pandas()\n",
    "\n",
    "        # Convert to pandas and create string format\n",
    "        text_data = \"\"\n",
    "        for _, row in data.iterrows():\n",
    "            text_data += f\"{row['Title']}: {row['Overview']}\\n\"\n",
    "\n",
    "        # Create character mappings\n",
    "        chars = sorted(list(set(text_data)))\n",
    "        self.string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "        self.int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "        # Encode text to integers\n",
    "        encoded_data = [self.string_to_int[c] for c in text_data]\n",
    "        \n",
    "        # Convert to tensor\n",
    "        self.data = torch.tensor(encoded_data, dtype=torch.long)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        x = chunk[:-1]  # all but last\n",
    "        y = chunk[1:]   # all but first\n",
    "        return x, y\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return ''.join([self.int_to_string[i.item()] for i in ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Dataset Content Analysis:\n",
      "Total characters in the dataset: 3002101\n",
      "Unique characters (vocabulary size): 169\n",
      "Example unique characters: ['\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\"]\n",
      "\n",
      "Most common characters:\n",
      "' ': 499859\n",
      "'e': 290022\n",
      "'t': 195126\n",
      "'a': 193505\n",
      "'o': 171696\n",
      "'i': 171218\n",
      "'n': 170269\n",
      "'r': 157127\n",
      "'s': 156990\n",
      "'h': 125681\n",
      "\n",
      "Least common characters:\n",
      "'`': 1\n",
      "'«': 1\n",
      "'»': 1\n",
      "'\t': 1\n",
      "'_': 1\n",
      "'‑': 1\n",
      "'£': 1\n",
      "'″': 1\n",
      "'Å': 1\n",
      "'ª': 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Sample Batch Analysis:\n",
      "Shape of input (x): torch.Size([10, 1000])\n",
      "Shape of target (y): torch.Size([10, 1000])\n",
      "\n",
      "Decoded Input (x): roup of mutants who are able to control and wield flames, and the fire disaster they have unleashed on Earth.\n",
      "Naruto: The Lost Story - Mission: Protect the Waterfall Village!: Naruto and his friends must get back a jug of stolen holy water from a band of higher class ninjas.\n",
      "The Boss Baby: A story about how a new baby's arrival impacts a family, told from the point of view of a delightfully unreliable narrator, a wildly imaginative 7 year old named Tim.\n",
      "Dolittle: After losing his wife seven years earlier, the eccentric Dr. John Dolittle, famed doctor and veterinarian of Queen Victoria’s England, hermits himself away behind the high walls of Dolittle Manor with only his menagerie of exotic animals for company. But when the young queen falls gravely ill, a reluctant Dolittle is forced to set sail on an epic adventure to a mythical island in search of a cure, regaining his wit and courage as he crosses old adversaries and discovers wondrous creatures.\n",
      "The Case for Christ: Based on the tru\n",
      "Decoded Target (y): oup of mutants who are able to control and wield flames, and the fire disaster they have unleashed on Earth.\n",
      "Naruto: The Lost Story - Mission: Protect the Waterfall Village!: Naruto and his friends must get back a jug of stolen holy water from a band of higher class ninjas.\n",
      "The Boss Baby: A story about how a new baby's arrival impacts a family, told from the point of view of a delightfully unreliable narrator, a wildly imaginative 7 year old named Tim.\n",
      "Dolittle: After losing his wife seven years earlier, the eccentric Dr. John Dolittle, famed doctor and veterinarian of Queen Victoria’s England, hermits himself away behind the high walls of Dolittle Manor with only his menagerie of exotic animals for company. But when the young queen falls gravely ill, a reluctant Dolittle is forced to set sail on an epic adventure to a mythical island in search of a cure, regaining his wit and courage as he crosses old adversaries and discovers wondrous creatures.\n",
      "The Case for Christ: Based on the true\n",
      "\n",
      "3. Block Size Impact Analysis:\n",
      "Block size controls the sequence length for input and target.\n",
      "Current block size: 1000\n",
      "Each input and target pair is 1000 characters long.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Instantiate the dataset\n",
    "block_size = 256  # A meaningful context length for modeling\n",
    "batch_size = 1000  # A large batch size to explore the data at scale\n",
    "dataset = MovieDataset(block_size=block_size)\n",
    "\n",
    "# PART 1: Analyze dataset structure (What does the dataset contain?)\n",
    "print(\"1. Dataset Content Analysis:\")\n",
    "print(f\"Total characters in the dataset: {len(dataset.data)}\")\n",
    "print(f\"Unique characters (vocabulary size): {len(dataset.string_to_int)}\")\n",
    "print(f\"Example unique characters: {list(dataset.string_to_int.keys())[:10]}\")\n",
    "\n",
    "# Count the frequency of each character\n",
    "char_counts = collections.Counter(dataset.data.tolist())\n",
    "most_common_chars = char_counts.most_common(10)\n",
    "least_common_chars = char_counts.most_common()[-10:]\n",
    "\n",
    "print(\"\\nMost common characters:\")\n",
    "for char, count in most_common_chars:\n",
    "    print(f\"'{dataset.int_to_string[char]}': {count}\")\n",
    "\n",
    "print(\"\\nLeast common characters:\")\n",
    "for char, count in least_common_chars:\n",
    "    print(f\"'{dataset.int_to_string[char]}': {count}\")\n",
    "\n",
    "# Plot character frequency distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar([dataset.int_to_string[char] for char, _ in most_common_chars],\n",
    "        [count for _, count in most_common_chars])\n",
    "plt.title(\"Top 10 Most Common Characters\")\n",
    "plt.xlabel(\"Character\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# PART 2: Fetch a sample batch and analyze (What does the dataloader return?)\n",
    "print(\"\\n2. Sample Batch Analysis:\")\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Fetch one batch\n",
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "    print(f\"Shape of input (x): {x.shape}\")  # (batch_size, block_size)\n",
    "    print(f\"Shape of target (y): {y.shape}\")  # (batch_size, block_size)\n",
    "\n",
    "    # Decode and display a subset of sequences for interpretability\n",
    "    for i in range(3):  # Show first 3 examples for clarity\n",
    "        print(f\"\\nDecoded Input (x[{i}]):\", dataset.decode(x[i]))\n",
    "        print(f\"Decoded Target (y[{i}]):\", dataset.decode(y[i]))\n",
    "    break\n",
    "\n",
    "# PART 3: Analyze Block Size Impact\n",
    "print(\"\\n3. Block Size Impact Analysis:\")\n",
    "print(f\"Block size controls the sequence length for input and target.\")\n",
    "print(f\"Current block size: {block_size}\")\n",
    "print(f\"Each input and target pair is {block_size} characters long.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int # Number of unique tokens in the vocabulary\n",
    "    block_size: int = 256 # Sequence length\n",
    "    n_block: int = 6 # Number of blocks in the transformer\n",
    "    n_head: int = 6 # Number of attention heads\n",
    "    n_embd: int = 384 # Embedding dimensionality\n",
    "    dropout: float = 0.2 # Dropout rate\n",
    "    bias: bool = True # If True, we add a bias to the LayerNorm and Linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0, f\"Embedding dimension {config.n_embd} must be divisible by number of heads {config.n_head}\"\n",
    "\n",
    "        self.n_head = config.n_head # Number of attention heads\n",
    "        self.n_embd = config.n_embd # Embedding dimensionality\n",
    "        self.dropout = config.dropout # Dropout rate\n",
    "\n",
    "        # Maps embedding into Q, K, V. We'll use one layer to generate these matrices for all heads at once.\n",
    "        self.qkv_map = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "\n",
    "        # After performing attention for each head individually, we concat the results \n",
    "        # and feed them through this linear layer.\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "\n",
    "        # Regularization\n",
    "        self.final_dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # Batch size, sequence length, n_embd\n",
    "        d_k = C // self.n_head # Dimension of the query, key, and value vectors (within a head)\n",
    "\n",
    "        # TODO: Implement Causal Self Attention\n",
    "        # Hint: The output of the qkv_map is a tensor of shape (B, T, 3*C).\n",
    "        # We need to split this tensor into Q, K, and V tensors of shape (B, T, C) each.\n",
    "        # Afterwards, reshape and transpose them to the correct shape (see assert statements),\n",
    "        # such that we have (smaller) Q, K, and V matrices for each head.\n",
    "        #DONE: Generate Q, K, V from input x\n",
    "        # The input x is passed through the qkv_map linear layer to produce Q, K, and V matrices.\n",
    "        # These matrices are concatenated into a single tensor of shape (B, T, 3*C).\n",
    "        qkv = self.qkv_map(x)  # Shape: (B, T, 3*C)\n",
    "        Q, K, V = torch.split(qkv, C, dim=2)  # Each of shape: (B, T, C)\n",
    "        # DONE: Reshape and transpose for multi-head attention\n",
    "        # The Q, K, and V matrices are reshaped to separate the multiple attention heads.\n",
    "        # They are transposed to bring the attention head dimension to the second position.\n",
    "        Q = Q.view(B, T, self.n_head, d_k).transpose(1, 2)  # Shape: (B, n_head, T, d_k)\n",
    "        K = K.view(B, T, self.n_head, d_k).transpose(1, 2)  # Shape: (B, n_head, T, d_k)\n",
    "        V = V.view(B, T, self.n_head, d_k).transpose(1, 2)  # Shape: (B, n_head, T, d_k)\n",
    "        for M in [Q, K, V]:\n",
    "            assert M.shape == (B, self.n_head, T, d_k), f\"Expected shape (B, self.n_head, T, d_k), but got {M.shape}\"\n",
    "\n",
    "        # TODO: Compute the attention weights and aggregated values as specified in the assignment sheet.\n",
    "        # Hint: Broadcasted matrix multiplication can be implemented using the @ operator.\n",
    "        # Hint: `torch.tril` may help you with masking the attention scores.\n",
    "        # DONE: Compute scaled dot-product attention with causal masking\n",
    "        # Attention scores are computed using the dot product of Q and K, scaled by the square root of d_k.\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "        # Causal mask ensures that tokens can only attend to themselves and previous tokens.\n",
    "        causal_mask = torch.tril(torch.ones(T, T, device=x.device))  # Shape: (T, T)\n",
    "        attn_scores = attn_scores.masked_fill(causal_mask == 0, float('-inf'))\n",
    "\n",
    "        # DONE: Softmax to get attention weights\n",
    "        # The attention scores are passed through a softmax to normalize them.\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)  # Shape: (B, n_head, T, T)\n",
    "\n",
    "        # DONE: Weighted sum of values\n",
    "        # The attention weights are used to compute a weighted sum of the V matrix.\n",
    "        aggregated_vals = torch.matmul(attn_weights, V)  # Shape: (B, n_head, T, d_k)\n",
    "        assert aggregated_vals.shape == (B, self.n_head, T, d_k), f\"Expected aggregated_vals shape (B, self.n_head, T, d_k), but got {aggregated_vals.shape}\"\n",
    "\n",
    "        # Combine all head outputs into the last dimension\n",
    "        out = aggregated_vals.transpose(1, 2).reshape(B, T, C)\n",
    "        out = self.proj(out) # This combines the outputs of all heads\n",
    "        out = self.final_dropout(out) # This is the final dropout layer\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test your implementation of the `CausalSelfAttention` class by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed successfully!\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = GPTConfig(vocab_size=10, block_size=8, n_block=6, n_head=6, n_embd=12, dropout=0.0, bias=True)\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "x = torch.randn(2, 8, 12).to(device)\n",
    "attention = CausalSelfAttention(config).to(device)\n",
    "att_out = attention(x)\n",
    "\n",
    "# Read expected output from file\n",
    "att_out_expected = torch.load('CausalSelfAttention_out.pt', map_location=device, weights_only=True)\n",
    "\n",
    "assert torch.allclose(att_out, att_out_expected, atol=1e-7), \"Outputs do not match!\"\n",
    "print(\"Test passed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # TODO: Implement the MLP\n",
    "        # It should consist of a linear layer, a GELU activation function, and a final linear layer.\n",
    "        # After the final linear layer, apply dropout with dropout rate config.dropout.\n",
    "        # The first linear layer should map from config.n_embd to 4 * config.n_embd.\n",
    "        # The second linear layer should map from 4 * config.n_embd back to config.n_embd.\n",
    "        # The linear layers should have a bias term if config.bias is True, and no bias term otherwise.\n",
    "         # First Linear Layer\n",
    "        # Expands the input dimensionality from `config.n_embd` to `4 * config.n_embd`.\n",
    "        # The bias term is controlled by `config.bias`.\n",
    "        self.fc1 = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # GELU Activation\n",
    "        # Applies the GELU (Gaussian Error Linear Unit) non-linearity.\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "        # Second Linear Layer\n",
    "        # Projects the expanded dimensionality `4 * config.n_embd` back to `config.n_embd`.\n",
    "        self.fc2 = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # Dropout\n",
    "        # Applies dropout after the final linear layer to prevent overfitting.\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass of the MLP\n",
    "        # Step 1: Apply the first linear transformation.\n",
    "        # This expands the input size from `n_embd` to `4 * n_embd`.\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        # Step 2: Apply the GELU activation function.\n",
    "        # Introduces non-linearity to the MLP block.\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        # Step 3: Apply the second linear transformation.\n",
    "        # Projects the expanded dimensionality back to the original size (`n_embd`).\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Step 4: Apply dropout.\n",
    "        # Prevents overfitting by randomly zeroing some elements of the output.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Step 5: Return the final output.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layernorm_1 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attention = CausalSelfAttention(config)\n",
    "        self.layernorm_2 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.layernorm_1(x))\n",
    "        x = x + self.mlp(self.layernorm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            embed_token = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            embed_position = nn.Embedding(config.block_size, config.n_embd),\n",
    "            dropout = nn.Dropout(config.dropout),\n",
    "            blocks = nn.ModuleList([Block(config) for _ in range(config.n_block)]),\n",
    "            layernorm = nn.LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # We use the same weights for the token embeddings and the final linear layer.\n",
    "        # This is a form of \"weight tying\", see https://paperswithcode.com/method/weight-tying\n",
    "        self.transformer.embed_token.weight = self.head.weight\n",
    "\n",
    "        # Initialize all linear layers using our custom init function\n",
    "        self.apply(self._init_params)\n",
    "\n",
    "        # report number of parameters\n",
    "        print(f\"Number of parameters in GPT: {self.get_num_params()/1e6:.2f}M\")\n",
    "\n",
    "\n",
    "    def _init_params(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    def get_num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.shape\n",
    "        assert t <= self.config.block_size, f\"Cannot process sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        position_idxs = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # DONE: Implement the forward pass of the GPT model\n",
    "        # Embed the tokens and positions using the embedding layers self.transformer.embed_token and self.transformer.embed_position.\n",
    "        # Add the token embeddings and position embeddings together and pass the result through the dropout layer.\n",
    "        # Pass the result through all the transformer blocks.\n",
    "        # Apply layer normalization and finally obtain the logits by project the result to \n",
    "        # the vocabulary space using the head layer.\n",
    " \n",
    "\n",
    "   \n",
    "\n",
    "        # Step 1: Token and positional embeddings\n",
    "        token_embeddings = self.transformer.embed_token(idx)  # Shape: (batch_size, seq_length, n_embd)\n",
    "    \n",
    "        position_embeddings = self.transformer.embed_position(position_idxs)  # Shape: (seq_length, n_embd)\n",
    "\n",
    "        # Combine token and positional embeddings, then apply dropout\n",
    "        x = self.transformer.dropout(token_embeddings + position_embeddings)\n",
    "\n",
    "        # Step 2: Pass through all transformer blocks\n",
    "        for block in self.transformer.blocks:\n",
    "            x = block(x)  # Each block applies attention, MLP, and residual connections\n",
    "\n",
    "        # Step 3: Apply final layer normalization\n",
    "        x = self.transformer.layernorm(x)  # Shape: (batch_size, seq_length, n_embd)\n",
    "\n",
    "        # Step 4: Project to vocabulary size using the head layer\n",
    "        logits = self.head(x)  # Shape: (batch_size, seq_length, vocab_size)\n",
    "        \n",
    "        if targets is not None:\n",
    "            # We calculate the loss if targets are provided (i.e., during training)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    def get_optimizer(self, weight_decay, learning_rate, betas, device):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "\n",
    "        # We will decay all parameters that are 2D or higher dimensional. \n",
    "        # This includes all weight matrices and embeddings.\n",
    "        decay_params = [p for n, p in param_dict.items() if len(p.shape) >= 2]\n",
    "        # We will not decay biases and layernorm parameters (which are 1D).\n",
    "        nodecay_params = [p for n, p in param_dict.items() if len(p.shape) < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        fused = (device == 'cuda')\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, fused=fused)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, idx, max_new_tokens, temperature=1.0):\n",
    "        # idx is of shape (batch_size, sequence_length)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # If the sequence context is growing too long we must crop it at block_size\n",
    "            idx_input = idx if idx.shape[1] <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # TODO: Push idx_input through the model to get the logits for the next token in the sequence\n",
    "            # Hint: The logits that are returned by the model are of shape (batch_size, sequence_length, vocab_size).\n",
    "            # To predict the next token, we only need the logits for the last position in the sequence.\n",
    "            # Next, divide the logits by the desired temperature and apply the softmax function to convert them to probabilities.\n",
    "            # Finally, sample the next token from this probability distribution.\n",
    "\n",
    "            next_token = ...\n",
    "            assert next_token.shape == (idx.shape[0], 1), f\"Expected next_token shape (batch_size, 1), but got {next_token.shape}\"\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_train_val_loss(model, train_loader, val_loader, val_iters, device):\n",
    "    model.eval()\n",
    "    losses = {}\n",
    "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "        total_loss = 0\n",
    "        for i, (X, Y) in enumerate(loader):\n",
    "            if i >= val_iters:\n",
    "                break\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            logits, loss = model(X, Y)\n",
    "            total_loss += loss.item()\n",
    "        losses[split] = total_loss / val_iters\n",
    "    model.train()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "block_size = 128\n",
    "batch_size = 128\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "data = MovieDataset(block_size)\n",
    "\n",
    "# split into train and validation sets\n",
    "train_len = int(len(data) * 0.8)\n",
    "val_len = len(data) - train_len\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(data, [train_len, val_len])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "out_dir = 'MovieGPT'\n",
    "checkpoint_path = os.path.join(out_dir, 'checkpoint.pt')\n",
    "os.makedirs(out_dir, exist_ok=True)  # Create output directory\n",
    "\n",
    "# Eval/Logging\n",
    "val_interval = 500 # Number of iterations between evaluations\n",
    "val_iters = 20 # Number of iterations for evaluation\n",
    "log_interval = 10 # Number of iterations between logging\n",
    "\n",
    "# Optimizer settings\n",
    "learning_rate = 1e-3 # Larger networks typically require a learning rate that is smaller than this\n",
    "max_iters = 5_000 # Number of iterations to train for\n",
    "weight_decay = 1e-1 # Weight decay for regularization (on the weights/embeddings)\n",
    "beta1, beta2 = 0.9, 0.99 # Beta1, Beta2 for AdamW optimizer\n",
    "grad_clip = 1.0 # Clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "# Compile model\n",
    "compile_model = True # Compile the model for faster execution\n",
    "\n",
    "# Model config\n",
    "vocab_size = ... # TODO: Use the dataset `data` to determine the vocabulary size\n",
    "config = GPTConfig(\n",
    "    block_size=block_size, \n",
    "    vocab_size=vocab_size, \n",
    "    n_block=4, \n",
    "    n_head=4, \n",
    "    n_embd=128, \n",
    "    dropout=0.0, \n",
    "    bias=False\n",
    ") # This is a relatively small model\n",
    "\n",
    "model = GPT(config).to(device)\n",
    "\n",
    "if compile_model:\n",
    "    print(\"Compiling the model...\")\n",
    "    model = torch.compile(model) # Needs PyTorch >= 2.0\n",
    "    print(\"Done compiling\")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = model.get_optimizer(weight_decay, learning_rate, (beta1, beta2), device)\n",
    "\n",
    "# Training loop\n",
    "iter_num = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for _ in range(max_iters):\n",
    "    for X, Y in train_loader:\n",
    "        # Get batch and move to device\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, loss = model(X, targets=Y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        if grad_clip != 0.0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        if iter_num % log_interval == 0:\n",
    "            print(f\"iter {iter_num}: loss {loss.item():.4f}\")\n",
    "            \n",
    "        # Evaluation\n",
    "        if iter_num % val_interval == 0:\n",
    "            losses = estimate_train_val_loss(model, train_loader, val_loader, val_iters, device)\n",
    "            print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if losses['val'] < best_val_loss:\n",
    "                best_val_loss = losses['val']\n",
    "                if iter_num > 0:\n",
    "                    print(f\"Saving checkpoint to {out_dir}\")\n",
    "                    model_to_save = model._orig_mod if compile_model else model\n",
    "                    torch.save({\n",
    "                        'model': model_to_save.state_dict(),\n",
    "                        'model_args': config,\n",
    "                    }, checkpoint_path)\n",
    "        \n",
    "        iter_num += 1\n",
    "        if iter_num >= max_iters:\n",
    "            break\n",
    "    \n",
    "    if iter_num >= max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5  # Number of samples to draw\n",
    "max_new_tokens = 500  # Number of tokens generated in each sample\n",
    "temperature = 0.8  # TODO: Use different temperature values and qualitatively report on the results\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 345  \n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Load the model\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "config = checkpoint['model_args']\n",
    "model = GPT(config)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Create dataset to get encoder/decoder\n",
    "dataset = MovieDataset(block_size=config.block_size)\n",
    "encode = lambda s: [dataset.string_to_int[c] for c in s]\n",
    "decode = dataset.decode\n",
    "\n",
    "# Generate samples\n",
    "print('-'*20)\n",
    "with torch.no_grad():\n",
    "    for k in range(num_samples):\n",
    "        start_prompt = \"\\n\"  # Start prompt\n",
    "        prompt_ids = encode(start_prompt)\n",
    "        x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n",
    "    \n",
    "        y = model.sample(x, max_new_tokens, temperature=temperature)\n",
    "        print(decode(y[0]))\n",
    "        print('-'*20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
